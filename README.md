<!-- Profile README for Mohd Ibrahim Afridi -->

<h1 align="center">💻 Mohd Ibrahim Afridi (Afridi)</h1>
<p align="center">
  <b>AI/ML Engineer • Independent Researcher • Entrepreneur • AI Safety & Trust</b><br>
  <a href="https://mohdibrahimai.github.io/portfolio-/">🌐 Portfolio</a> ·
  <a href="mailto:mohdibrahimafridi.ai@gmail.com">📧 Email</a> ·
  <a href="https://www.linkedin.com/in/mohd-ibrahim-afridi-381b12381">💼 LinkedIn</a> ·
  <a href="https://github.com/mohdibrahimai">🐙 GitHub</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/focus-AI%20Safety%20%26%20Trust-4C1" alt="Focus">
  <img src="https://img.shields.io/badge/remote-Ready-blue" alt="Remote">
  <img src="https://img.shields.io/badge/languages-EN%20%7C%20HI%20%7C%20UR%20%7C%20ES-ff69b4" alt="Languages">
  <img src="https://komarev.com/ghpvc/?username=mohdibrahimai&label=Profile%20views&color=0e75b6&style=flat" alt="views">
</p>

---

## 🚀 About Me
Hi, I’m Afridi — I build **verifiable, trustworthy, and safe AI systems**.  
- 🧠 Founder & CTO at **XCL3NT**, an AI-first commerce brand  
- 🧪 Researcher behind **Dynamic Chain-of-Thought Reward Models (D-CoT)** — [Read the preprint](https://zenodo.org/records/16554886)  
- 🛡️ Obsession: **Evidence-bound answers, eval frameworks, and safety gates**  
- 🌍 Remote-ready; open to relocation (US/EU/NZ/SEA)

> *Mission: Make AI safer, more transparent, and actually helpful.* 🌱

---

## 🧪 Research & Interests
- **AI Safety & Trust:** refusal/routing policies, post-hoc verification, red-teaming  
- **Verifiable QA:** claims must cite sources; sentence-level checks  
- **Model Behavior:** failure modes, calibration, alignment metrics  
- **Evaluation:** SWA/NER/CR/ECE, CI gates, nightly reports  
- **Human-Data Pipelines:** collection → curation → evals  
- **Freshness/Cost Routing:** retrieval-generation hybrids, gating

---

## 🛠️ Selected Projects
| Project | What it does | Stack |
| --- | --- | --- |
| [**ARGOS**](https://github.com/mohdibrahimai/ARGOS) | Evidence-bound answering (retriever → answer → verifier) with inline citations & dashboards | FastAPI, Next.js, Docker, Helm |
| [**HELMSMAN**](https://github.com/mohdibrahimai/HELMSMAN) | **Prompt contracts + fuzzing CI** (contracts DSL, packs, diff & gates) | Python, YAML |
| [**PALADIN**](https://github.com/mohdibrahimai/PALADIN) | **Proof-carrying answers** via minimal evidence graphs + deterministic verifier | Python |
| [**UIRE**](https://github.com/mohdibrahimai/UIRE) | **Universal Intent Resolution** (ambiguity detection → micro-clarify → policy) | FastAPI, Docker, Helm |
| [**JANUS**](https://github.com/mohdibrahimai/JANUS) | **Parametric-vs-Retrieval Gating** (freshness/cost-aware decisions) | Python, Policy Engine |
| [**TruthLens**](https://huggingface.co/spaces/afridi/TruthLens) | Claim → Evidence stance (support/contradict/neutral) | HF Spaces, Transformers |

> Also: DataLoaderSpeedrun, BreezeMind-Pro, Career Vision AI, Human-Feedback-Safety-Simulator — see my GitHub.

---

## 📈 Impact Highlights
- 🔎 Hallucinations ↓ **38%**, latency ↓ **23%**, cost ↓ **44%** across 5+ pipelines  
- 🎯 Factual F1 ↑ **7–12pp**; ArenaHard alignment ↑ **+3.4pp** with D-CoT RMs  
- 📚 Research artifacts: [Grok-3](https://zenodo.org/records/15227014), [Grok-3+](https://zenodo.org/records/15341810)  
- 🧪 “Behavior as tests”: contracts + packs + nightly eval dashboards + go/no-go gates

---

## 🧠 Tech Stack
**Languages:** Python, C++, TypeScript/JS  
**Frameworks:** PyTorch, TensorFlow, JAX, FastAPI, Next.js  
**Infra:** Docker, Kubernetes, Helm, Prometheus, Grafana  
**Concepts:** MoE, FP8, RLHF, KV Caching, LoRA, DQN  
**Other:** Retrieval/RAG, Z3, Lean4, CI/CD, eval pipelines

---

## 🔒 Safety Principles (How I build)
- **Evidence-bound** by default (each claim cites sources)  
- **Prompt contracts** + **CI gates** to catch regressions early  
- **Nightly evals** tracking truthfulness & calibration  
- Prefer **abstain/route** over confident nonsense  
- Ship **receipts**: versions, seeds, costs, checks (replayable)

---

## 🤝 Let’s Collaborate
Working on **frontier models, eval frameworks, or safety tooling**? I’d love to jam.  
**Email:** mohdibrahimafridi.ai@gmail.com — I reply fast (unless it’s 3 AM and I’m arguing with a loss curve).

> *“AI safety isn’t a checkbox — it’s a responsibility.”* — me (and my coffee) ☕

---

## 📊 GitHub Stats
<p align="left">
  <img src="https://github-readme-stats.vercel.app/api?username=mohdibrahimai&show_icons=true&theme=radical" alt="Afridi's GitHub stats">
</p>
<!-- Optional: add top-langs if you want
<p align="left">
  <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=mohdibrahimai&layout=compact&theme=radical" alt="Top Langs">
</p>
-->
