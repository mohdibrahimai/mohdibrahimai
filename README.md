# ğŸ’» Mohd Ibrahim Afridi (Afridi)

**AI/ML Engineer â€¢ Independent Researcher â€¢ Entrepreneur â€¢ AI Safety & Trust**
[ğŸŒ Portfolio](https://mohdibrahimai.github.io/portfolio-/) Â· [ğŸ“§ Email](mailto:mohdibrahimafridi.ai@gmail.com) Â· [ğŸ’¼ LinkedIn](https://www.linkedin.com/in/mohd-ibrahim-afridi-381b12381) Â· [ğŸ™ GitHub](https://github.com/mohdibrahimai)

---

## ğŸš€ About Me

Hi, Iâ€™m Afridi â€” an AI/ML engineer and independent researcher obsessed with building **verifiable, trustworthy, safe AI systems**.

* ğŸ§  Founder & CTO at **XCL3NT**, an AIâ€‘first commerce brand
* ğŸ¤– Researcher behind **Dynamic Chainâ€‘ofâ€‘Thought Reward Models (Dâ€‘CoT)** â€” [Read Dâ€‘CoT](https://zenodo.org/records/16554886)
* ğŸ›¡ï¸ Focus: **AI Safety & Trust** â€” designing systems with evidence, evaluation, and safeguards by default
* ğŸŒ Remoteâ€‘ready and open to relocation (US/EU/NZ/SEA)

> *Mission: Make AI safer, more transparent, and actually helpful to humanity.* ğŸŒ±

---

## ğŸ§ª Research Focus

* **AI Safety & Trust:** principled safeguards, routing/abstain policies, postâ€‘hoc verification
* **Evidenceâ€‘bound QA:** answers backed by citations + sentenceâ€‘level verification
* **Model behavior & evals:** metrics, gates, and nightly reports that catch regressions
* **Humanâ€‘data pipelines:** collection â†’ curation â†’ evaluation
* **Freshness/cost routing:** parametric vs retrieval vs compute

---

## ğŸ› ï¸ Selected Projects (renamed & polished)

| Project (Display Name)                                                     | Repo / Link                                                                                      | What it Does                                                                           | Stack                                              |
| -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------- | -------------------------------------------------- |
| **Evidenceâ€‘Bound Answering System** *(formerly ARGOS)*                     | [https://github.com/mohdibrahimai/ARGOS](https://github.com/mohdibrahimai/ARGOS)                 | Retriever â†’ Answer â†’ **Verifier** with inline citations, metrics, and dashboards.      | FastAPI, Next.js, Docker, Helm, Prometheus/Grafana |
| **Prompt Contracts + Fuzzing CI for Answer Engines** *(formerly HELMSMAN)* | [https://github.com/mohdibrahimai/HELMSMAN](https://github.com/mohdibrahimai/HELMSMAN)           | Turn prompts into **contracts**, run packs, gate regressions in CI.                    | Python, YAML DSL, scikitâ€‘learn                     |
| **VerifyQA â€” Proofâ€‘Carrying Answers** *(formerly PALADIN)*                 | [https://github.com/mohdibrahimai/PALADIN](https://github.com/mohdibrahimai/PALADIN)             | Minimal **evidence graphs** + deterministic verifier; trains a proofâ€‘planner.          | Python, Streamlit                                  |
| **Humanâ€‘Guided Parametricâ€‘vsâ€‘Retrieval Gating** *(formerly JANUS)*         | [https://github.com/mohdibrahimai/JANUS](https://github.com/mohdibrahimai/JANUS)                 | Policy decides: answer from memory, **retrieve & cite**, compute, clarify, or abstain. | Python, FastAPI, Torch (policy)                    |
| **UIRE â€” Universal Intent Resolution Engine**                              | [https://github.com/mohdibrahimai/UIRE](https://github.com/mohdibrahimai/UIRE)                   | Detect ambiguity â†’ microâ€‘clarify â†’ produce executable intents.                         | FastAPI, Docker, Helm                              |
| **TruthLens** (HF Space)                                                   | [https://huggingface.co/spaces/afridi/TruthLens](https://huggingface.co/spaces/afridi/TruthLens) | Claim â†’ Evidence stance (support/contradict/neutral).                                  | HF Spaces, Transformers                            |

> More: DataLoaderSpeedrun, BreezeMindâ€‘Pro, Career Vision AI, Humanâ€‘Feedbackâ€‘Safetyâ€‘Simulator, etc. on my GitHub.

---

## ğŸ“ˆ Impact Highlights

* ğŸ“Š Reduced hallucinations by **âˆ’38%**, latency by **âˆ’23%**, and cost by **âˆ’44%** across 5+ pipelines
* ğŸ† Improved factual F1 by **+7â€“12pp**; **Dâ€‘CoT RMs** raised ArenaHard by **+3.4pp** and cut CoT latency
* ğŸ“š Research artifacts: [Grokâ€‘3](https://zenodo.org/records/15227014), [Grokâ€‘3+](https://zenodo.org/records/15341810), [Dâ€‘CoT](https://zenodo.org/records/16554886)
* ğŸ§© Operationalized **prompt contracts**, nightly eval dashboards, and safety **gates**

---

## ğŸ§  Tech Stack

**Languages:** Python, C++, TypeScript, JS
**Frameworks:** PyTorch, TensorFlow, JAX, FastAPI, Next.js
**Infra:** Docker, Kubernetes, Helm, Prometheus, Grafana
**Concepts:** MoE, FP8, RLHF, KV caching, LoRA, DQN
**Other:** Z3, Lean4, CI/CD, Retrieval, Eval pipelines

---

## ğŸ”’ AI Safety & Trust Principles

* Build **evidenceâ€‘bound** systems (claims must cite sources)
* Add **prompt contracts** + **CI gates** to prevent regressions
* Run **nightly evals** (truthfulness, calibration, safety)
* Prefer **abstain/route** over confident nonsense
* Ship **receipts**: versions, seeds, costs, checks for replayability

---

## ğŸ’Œ Letâ€™s Collaborate

If youâ€™re building **frontier models, eval frameworks, or safety tooling** â€” Iâ€™d love to collaborate.
Letâ€™s make AI **safer, smarter, and actually trustworthy.** ğŸ›¡ï¸

> *â€œAI safety isnâ€™t a checkbox â€” itâ€™s a responsibility.â€* â€“ Me, probably during a caffeine high â˜•ğŸ˜„

---

## ğŸ“Š GitHub Stats

![Afridi's GitHub stats](https://github-readme-stats.vercel.app/api?username=mohdibrahimai\&show_icons=true\&theme=radical)

---

## ğŸ§© Fun Fact

I treat debugging like detective workâ€¦ except the culprit is **me** from 3 AM last night. ğŸ•µï¸â€â™‚ï¸
